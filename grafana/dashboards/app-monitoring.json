{
  "id": null,
  "title": "RAG-Api Monitoring",
  "timezone": "browser",
  "schemaVersion": 30,
  "version": 1,
  "refresh": "5s",
  "panels": [
    {
      "type": "stat",
      "title": "Request Rate (req/s)",
      "id": 1,
      "gridPos": { "h": 8, "w": 6, "x": 0, "y": 0 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "rate(http_requests_total[1m])",
          "legendFormat": "req/s",
          "interval": "",
          "refId": "A"
        }
      ],
      "fieldConfig": {
        "defaults": {
          "unit": "reqps",
          "color": { "mode": "thresholds" },
          "thresholds": {
            "mode": "absolute",
            "steps": [
              { "color": "green", "value": null },
              { "color": "yellow", "value": 10 },
              { "color": "red", "value": 50 }
            ]
          }
        },
        "overrides": []
      }
    },
    {
      "type": "timeseries",
      "title": "Response Time (Latency)",
      "id": 2,
      "gridPos": { "h": 8, "w": 12, "x": 6, "y": 0 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, rate(request_duration_seconds_bucket[1m]))",
          "legendFormat": "95th percentile",
          "refId": "A"
        },
        {
          "expr": "histogram_quantile(0.50, rate(request_duration_seconds_bucket[1m]))",
          "legendFormat": "50th percentile",
          "refId": "B"
        }
      ]
    },
    {
      "type": "timeseries",
      "title": "CPU Usage (%)",
      "id": 3,
      "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "cpu_usage_percent",
          "legendFormat": "CPU %",
          "refId": "A"
        }
      ]
    },
    {
      "type": "timeseries",
      "title": "Memory Usage (%)",
      "id": 4,
      "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "memory_usage_percent",
          "legendFormat": "Memory %",
          "refId": "A"
        }
      ]
    },
    {
        "type": "graph",
        "title": "LLM Inference Duration (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(llm_inference_duration_seconds_bucket[1m]))",
            "legendFormat": "{{model}} (chat)",
            "refId": "A"
          }
        ],
        "datasource": "Prometheus",
        "gridPos": { "x": 0, "y": 0, "w": 24, "h": 9 }
      },
      {
        "type": "graph",
        "title": "LLM Stream Inference Duration (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(llm_stream_duration_seconds_bucket[1m]))",
            "legendFormat": "{{model}} (stream)",
            "refId": "B"
          }
        ],
        "datasource": "Prometheus",
        "gridPos": { "x": 0, "y": 9, "w": 24, "h": 9 }
      },
      {
        "type": "graph",
        "title": "LLM Inference Duration (Average)",
        "targets": [
          {
            "expr": "rate(llm_inference_duration_seconds_sum[1m]) / rate(llm_inference_duration_seconds_count[1m])",
            "legendFormat": "{{model}} (avg)",
            "refId": "C"
          }
        ],
        "datasource": "Prometheus",
        "gridPos": { "x": 0, "y": 18, "w": 24, "h": 9 }
      },
      {
        "type": "graph",
        "title": "LLM Inference Request Count",
        "targets": [
          {
            "expr": "rate(llm_inference_duration_seconds_count[1m])",
            "legendFormat": "{{model}}",
            "refId": "D"
          }
        ],
        "datasource": "Prometheus",
        "gridPos": { "x": 0, "y": 27, "w": 24, "h": 9 }
      }
  ]
}